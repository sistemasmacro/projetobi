[core]
hostname_callable = socket.getfqdn

# Test connection is typically a debug feature, ensure it's intentionally enabled.
test_connection = Enabled

# Uncomment the absolute path for your DAGs folder.
dags_folder = /opt/airflow/dags

# Set timezone to America/Sao_Paulo (BRST/BRT depending on DST)
default_timezone = America/Sao_Paulo

# CeleryExecutor is used for distributed task execution.
executor = CeleryExecutor

# Database connection settings.
sql_engine_encoding = utf-8
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 10
sql_alchemy_max_overflow = 20
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True

# Parallelism and concurrency settings.
parallelism = 2
max_active_tasks_per_dag = 2

# DAG configuration.
dags_are_paused_at_creation = False
non_pooled_task_slot_count = 128
load_examples = False

# XCom pickling (deprecated) should be disabled for security reasons.
enable_xcom_pickling = False

# Hide sensitive variables and connections fields in UI and logs.
hide_sensitive_var_conn_fields = True

dag_concurrency = 4

[logging]
retention_days = 7  # Log retention of 7 days, adjust as needed.
base_log_folder = /opt/airflow/logs
logging_level = INFO
fab_logging_level = WARN
colored_console_log = True

# Log formatting for better readability.
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

[api]
auth_backend = airflow.api.auth.backend.basic_auth

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080
web_server_master_timeout = 120
web_server_worker_timeout = 120
workers = 4
worker_refresh_batch_size = 1
worker_refresh_interval = 60

# Security settings.
secret_key = {SECRET_KEY}
expose_config = False
authenticate = False
filter_by_owner = False
owner_mode = user
dag_default_view = tree
dag_orientation = LR
session_lifetime_minutes = 120

[celery]
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://:@redis:6379/0
flower_host = 0.0.0.0
flower_port = 5555
sync_parallelism = 0
ssl_active = False

[celery_broker_transport_options]
visibility_timeout = 21600

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
run_duration = -1
num_runs = -1
scheduler_idle_sleep_time = 1
min_file_process_interval = 60
dag_dir_list_interval = 60
print_stats_interval = 30
scheduler_health_check_threshold = 30
child_process_log_directory = /opt/airflow/logs/scheduler

[smtp]
smtp_host = sandbox.smtp.mailtrap.io
smtp_starttls = True
smtp_ssl = False
smtp_user = 3c060547ef5c51
smtp_password = f86e9d7e6ec271
smtp_port = 587
smtp_mail_from = airflow@macromaq.com 